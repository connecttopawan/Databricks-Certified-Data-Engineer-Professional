# Detailed Databricks Tooling Notes for Data Engineer Professional Certification

These notes are designed to help you prepare for the **Databricks Certified Data Engineer Professional** exam by explaining key Databricks tooling concepts in simple, easy-to-understand language. Each section covers a topic with practical details, examples, and tips to ensure you grasp the essentials.

---

## 1. Understand and Navigate the Databricks Platform and Workspace

### What is the Databricks Workspace?
The Databricks workspace is your central hub on the Databricks platform. Think of it as a digital workspace where you and your team can:
- Create and store **notebooks** (for coding), **datasets**, **jobs**, and other resources.
- Organize everything into folders, like a file system on your computer.
- Collaborate by sharing folders or notebooks with teammates, with permission controls.

### Key Components of the Workspace
- **Home:** Your personal area for notebooks, folders, and personal files.
- **Data (Catalog):** A place to browse and manage databases, tables, views, and files stored in the Databricks File System (DBFS) or external storage.
- **Compute:** Where you manage clusters (the computing power for running your code).
- **Workflows:** For scheduling and running automated jobs or pipelines.
- **SQL:** For writing SQL queries, creating dashboards, and building alerts.
- **Repos:** For integrating with Git to manage code versions.
- **Machine Learning:** For building and deploying ML models (if enabled).

### Navigating the Workspace
- **Sidebar:** The left-hand menu is your navigation tool. Click icons to jump to:
  - **Home** for notebooks and folders.
  - **Compute** for clusters.
  - **Workflows** for job orchestration.
  - **SQL** for queries and dashboards.
- **Search Bar:** At the top, search for notebooks, tables, or jobs by name.
- **Switching Workspaces:** If you have access to multiple workspaces, click your profile (top-right) > "Workspaces" > select the desired workspace.
- **Permissions:** Right-click folders or notebooks to set who can view, edit, or run them.

### Practical Tips
- Use folders to organize projects (e.g., `/Users/yourname/project1`).
- Check your **entitlements** (permissions) to see what actions you can perform, like creating clusters or managing users.
- Import notebooks by right-clicking a folder > "Import" > upload a `.dbc`, `.py`, `.sql`, or other supported file.

### Why It Matters
Understanding the workspace layout helps you quickly find and manage resources, collaborate with your team, and stay organized during development and production.

---

## 2. Configure and Manage Databricks Clusters, Libraries, and Notebooks

### Clusters
- **What are they?** Clusters are groups of virtual machines (computers) that run your Databricks jobs. They consist of:
  - **Driver Node:** The "boss" that coordinates tasks.
  - **Worker Nodes:** The "workers" that process data in parallel.
- **Types of Clusters:**
  - **Single-node:** Only a driver node, ideal for small tasks like testing or lightweight development.
  - **Multi-node:** Driver + multiple workers, used for big data processing or production jobs.
- **Creating a Cluster:**
  - Go to **Compute** > **Create Cluster** in the UI.
  - Key settings:
    - **Cluster Name:** Give it a meaningful name (e.g., `DataProcessing_Prod`).
    - **Cluster Mode:**
      - **Standard:** General-purpose, good for most tasks.
      - **High Concurrency:** Optimized for multiple users sharing the cluster (e.g., SQL queries).
    - **Databricks Runtime Version:** Choose a version (e.g., 14.3 LTS) for Spark and library compatibility.
    - **Node Type:** Select VM size (e.g., Standard_DS3_v2) based on memory/CPU needs.
    - **Autoscaling:** Enable to automatically add/remove workers based on workload (saves costs).
    - **Spot Instances:** Use cheaper, interruptible VMs for non-critical jobs.
    - **Auto-termination:** Set an idle time (e.g., 30 minutes) to shut down the cluster and save costs.
- **Managing Clusters:**
  - Start/stop clusters manually in the Compute tab.
  - Use **cluster policies** (set by admins) to enforce rules, like limiting VM types or max workers.
  - Monitor cluster metrics (CPU, memory) in the cluster UI to optimize performance.

### Libraries
- **What are they?** Libraries are external packages or dependencies (e.g., Python’s `pandas`, Java’s JAR files) that add functionality to your clusters.
- **Installing Libraries:**
  - In the cluster UI, go to **Libraries** tab > **Install New**.
  - Types:
    - **PyPI:** For Python packages (e.g., `requests==2.28.1`).
    - **Maven:** For Java/Scala libraries (e.g., Spark connectors).
    - **JAR/Wheel:** Upload custom files.
  - **Workspace-level Libraries:** Install at the workspace level (via Libraries UI) to share across multiple clusters.
- **Tip:** Restart the cluster after installing libraries to ensure they load properly.

### Notebooks
- **What are they?** Notebooks are interactive documents where you write code (Python, Scala, SQL, R), visualize results, and add comments using markdown.
- **Key Actions:**
  - **Create:** File > New Notebook, select language and cluster.
  - **Attach to Cluster:** Use the dropdown at the top of the notebook to attach a running cluster.
  - **Clone:** File > Clone to create a copy for testing without changing the original.
  - **Permissions:** Right-click > Permissions to set who can view, edit, or run the notebook.
  - **Run Cells:** Use `Shift+Enter` to execute code cells or `%run /path/to/notebook` to run another notebook.
- **Tip:** Use markdown cells (`%md`) for documentation and `%sql` for SQL queries within notebooks.

### Practical Tips
- Use single-node clusters for development to save costs, multi-node for production.
- Regularly check library versions for compatibility with your runtime.
- Organize notebooks in folders and use clear names (e.g., `ETL_Pipeline_v1`).

### Why It Matters
Clusters, libraries, and notebooks are the backbone of running data processing tasks. Proper configuration ensures efficient, cost-effective, and reliable workflows.

---

## 3. Utilize Databricks CLI and REST API for Automation and Management

### Databricks CLI
- **What is it?** A command-line tool to manage Databricks resources (clusters, jobs, notebooks) without using the UI, perfect for automation scripts.
- **Setup:**
  - Install: Run `pip install databricks-cli` in your terminal.
  - Configure: Run `databricks configure --token`, then provide:
    - **Databricks Host:** Your workspace URL (e.g., `https://<region>.azuredatabricks.net`).
    - **Token:** A personal access token from User Settings > Access Tokens.
- **Common Commands:**
  - List clusters: `databricks clusters list` (shows cluster IDs, names, statuses).
  - Start cluster: `databricks clusters start --cluster-id <id>`.
  - Upload notebook: `databricks workspace import --path /local/notebook.py --destination /Workspace/path`.
  - Export folder: `databricks workspace export_dir /Workspace/folder /local/dir`.
- **Tip:** Add `--output JSON` for structured output in scripts (e.g., for parsing in Python).

### Databricks REST API
- **What is it?** A programmatic way to interact with Databricks using HTTP requests, ideal for integrating with CI/CD pipelines or custom apps.
- **Authentication:** Use a personal access token in the request header: `Authorization: Bearer <token>`.
- **Key Endpoints:**
  - **Clusters API:** `/api/2.0/clusters/list` to list clusters, `/api/2.0/clusters/start` to start one.
  - **Jobs API:** `/api/2.1/jobs/create` to create a job, `/api/2.1/jobs/run-now` to trigger it.
  - **Workspace API:** `/api/2.0/workspace/import` to upload notebooks, `/api/2.0/workspace/export` to download.
  - **Secrets API:** `/api/2.0/secrets/put` to store sensitive data like API keys.
- **Example (Python):**
  ```python
  import requests
  url = "https://<region>.azuredatabricks.net/api/2.0/clusters/list"
  headers = {"Authorization": "Bearer <token>"}
  response = requests.get(url, headers=headers)
  print(response.json())
  ```

### Practical Tips
- Use CLI for quick tasks like uploading notebooks in bulk.
- Use REST API for complex automation, like triggering jobs from a CI/CD pipeline.
- Store tokens securely (e.g., in environment variables) to avoid exposing them.

### Why It Matters
CLI and REST API enable automation, reducing manual work and ensuring consistency in managing large-scale Databricks environments.

---

## 4. Use dbutils for File and Dependency Management

### What is dbutils?
- `dbutils` is a built-in utility module in Databricks notebooks for tasks like file operations, secret management, and widget handling.
- Available in Python, Scala, and R; access it with `dbutils.help()` to see available commands.

### File Management with dbutils.fs
- **Databricks File System (DBFS):** A distributed file system for storing data, notebooks, and libraries.
- **Key Commands:**
  - List files: `dbutils.fs.ls("/path/to/dir")` – Returns file names, sizes, and paths.
  - Copy files: `dbutils.fs.cp("/source", "/destination", recurse=True)` – Copies files or folders.
  - Move files: `dbutils.fs.mv("/source", "/destination")` – Moves files or folders.
  - Delete: `dbutils.fs.rm("/path", recurse=True)` – Deletes files or directories.
  - Mount storage: `dbutils.fs.mount(source="s3://bucket", mountPoint="/mnt/dir", extraConfigs={"fs.s3a.access.key": key})` – Connects cloud storage (e.g., S3, ADLS) to DBFS.
- **Example:**
  ```python
  # List files in DBFS
  display(dbutils.fs.ls("/mnt/data"))
  # Copy a file
  dbutils.fs.cp("/mnt/data/input.csv", "/mnt/data/backup/input.csv")
  ```

### Dependency Management with dbutils.widgets
- **What are widgets?** Interactive inputs (e.g., text boxes, dropdowns) for passing parameters to notebooks.
- **Key Commands:**
  - Create: `dbutils.widgets.text("param_name", "default_value", "Label")` – Creates a text input.
  - Get value: `param = dbutils.widgets.get("param_name")` – Retrieves the input value.
  - Dropdown: `dbutils.widgets.dropdown("env", "prod", ["dev", "prod"], "Environment")` – Creates a dropdown.
- **Example:**
  ```python
  dbutils.widgets.text("table_name", "sales", "Table Name")
  table = dbutils.widgets.get("table_name")
  spark.sql(f"SELECT * FROM {table}").show()
  ```

### Secrets Management
- **Purpose:** Securely store and access sensitive data (e.g., API keys, passwords).
- **Commands:**
  - Get secret: `dbutils.secrets.get(scope="my_scope", key="my_key")`.
  - Scopes are created via CLI/API (e.g., `databricks secrets create-scope my_scope`).
- **Example:**
  ```python
  api_key = dbutils.secrets.get(scope="api_scope", key="api_key")
  ```

### Practical Tips
- Use `dbutils.fs.mount` to connect cloud storage once and access it like a local folder.
- Use widgets for reusable notebooks that need dynamic inputs (e.g., table names).
- Always use secrets for sensitive data instead of hardcoding in notebooks.

### Why It Matters
`dbutils` simplifies file operations and dependency management, making your notebooks more flexible and secure.

---

## 5. Implement Databricks Workflows for Job and Task Orchestration

### What are Databricks Workflows?
- Workflows are a way to automate and orchestrate multiple tasks (e.g., running notebooks, JARs, or SQL queries) in a specific order.
- Unlike a single job (one task), workflows support complex pipelines with dependencies.

### Creating a Workflow
- **Steps:**
  - Go to **Workflows** > **Create Job**.
  - Add tasks:
    - **Task Type:** Notebook, JAR, Python script, Delta Live Tables, etc.
    - **Cluster:** Choose an existing cluster or create a new one.
    - **Dependencies:** Set which tasks run after others (e.g., Task B depends on Task A).
  - Name tasks clearly (e.g., `Extract_Data`, `Transform_Data`).
- **Example Workflow:**
  - Task 1: Extract data from a CSV (notebook).
  - Task 2: Transform data with Spark (depends on Task 1).
  - Task 3: Load to Delta table (depends on Task 2).

### Scheduling Workflows
- In the job UI, click **Schedule**:
  - Use cron syntax (e.g., `0 0 * * *` for daily at midnight).
  - Set time zone and pause/resume options.
- **Tip:** Test the workflow manually before scheduling to ensure it runs correctly.

### Advanced Features
- **Task Values:** Pass data between tasks using `dbutils.jobs.taskValues.set(key, value)` and `{{tasks.task_name.values.key}}`.
  ```python
  # In Task 1
  dbutils.jobs.taskValues.set("row_count", 100)
  # In Task 2
  row_count = "{{tasks.task1.values.row_count}}"
  ```
- **Repair Runs:** Re-run failed tasks with modified parameters without restarting the entire workflow.
- **Alerts:** Set email notifications for success, failure, or skipped tasks in the job settings.

### Delta Live Tables (DLT) in Workflows
- DLT is a special workflow type for building ETL pipelines using declarative SQL or Python.
- Create via **Workflows** > **Delta Live Tables** > define datasets and transformations.
- **Example:** Define a pipeline to clean and aggregate data into a Delta table.

### Practical Tips
- Break complex jobs into smaller tasks for easier debugging.
- Use repair runs to fix failures without re-running successful tasks.
- Monitor runs in the Workflows UI (logs, durations, and errors).

### Why It Matters
Workflows automate and orchestrate data pipelines, ensuring reliable execution and scalability for production jobs.

---

## 6. Manage Databricks Repos for Version Control and Collaboration

### What are Databricks Repos?
- Repos integrate Databricks with Git (e.g., GitHub, GitLab) for version control of notebooks, scripts, and other files.
- They allow teams to collaborate, track changes, and manage code versions.

### Setting Up Repos
- **Steps:**
  - Go to **Repos** in the sidebar > **Add Repo**.
  - Provide:
    - **Git URL:** Your repository URL (e.g., `https://github.com/user/repo`).
    - **Provider:** GitHub, GitLab, Bitbucket, etc.
    - **Branch:** The Git branch to sync (e.g., `main`).
  - Authenticate with a Git token (generate in your Git provider’s settings).
- **Result:** The repo appears as a folder in `/Repos/yourname/repo`.

### Using Repos
- **Clone and Sync:**
  - Clone a Git repo into Databricks to work on notebooks and files.
  - Pull updates: Right-click the repo > **Pull** to sync with the remote Git branch.
  - Commit/push: Edit files, commit changes in the UI, and push to Git.
- **Branching:**
  - Create branches in the Repos UI to work on features or fixes.
  - Switch branches via the repo’s branch dropdown.
- **Collaboration:**
  - Multiple users can work on the same repo, with Git managing conflicts.
  - Use pull requests (in your Git provider) for code reviews before merging.

### Practical Tips
- Store notebooks as `.py` or `.sql` files in Repos for better Git compatibility (instead of `.dbc`).
- Regularly pull updates to avoid conflicts with teammates’ changes.
- Use Repos for CI/CD by integrating with GitHub Actions or similar tools.

### Why It Matters
Repos enable version control and collaboration, ensuring code quality and consistency in team projects.

