{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4d2K9m1UiOQQxSHJKYhGX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Understand and Navigate the Databricks Platform and Workspace\n",
        "\n",
        "### What is the Databricks Workspace?\n",
        "The Databricks workspace is your central hub on the Databricks platform. Think of it as a digital workspace where you and your team can:\n",
        "- Create and store **notebooks** (for coding), **datasets**, **jobs**, and other resources.\n",
        "- Organize everything into folders, like a file system on your computer.\n",
        "- Collaborate by sharing folders or notebooks with teammates, with permission controls.\n",
        "\n",
        "### Key Components of the Workspace\n",
        "- **Home:** Your personal area for notebooks, folders, and personal files.\n",
        "- **Data (Catalog):** A place to browse and manage databases, tables, views, and files stored in the Databricks File System (DBFS) or external storage.\n",
        "- **Compute:** Where you manage clusters (the computing power for running your code).\n",
        "- **Workflows:** For scheduling and running automated jobs or pipelines.\n",
        "- **SQL:** For writing SQL queries, creating dashboards, and building alerts.\n",
        "- **Repos:** For integrating with Git to manage code versions.\n",
        "- **Machine Learning:** For building and deploying ML models (if enabled).\n",
        "\n",
        "### Navigating the Workspace\n",
        "- **Sidebar:** The left-hand menu is your navigation tool. Click icons to jump to:\n",
        "  - **Home** for notebooks and folders.\n",
        "  - **Compute** for clusters.\n",
        "  - **Workflows** for job orchestration.\n",
        "  - **SQL** for queries and dashboards.\n",
        "- **Search Bar:** At the top, search for notebooks, tables, or jobs by name.\n",
        "- **Switching Workspaces:** If you have access to multiple workspaces, click your profile (top-right) > \"Workspaces\" > select the desired workspace.\n",
        "- **Permissions:** Right-click folders or notebooks to set who can view, edit, or run them.\n",
        "\n",
        "### Practical Tips\n",
        "- Use folders to organize projects (e.g., `/Users/yourname/project1`).\n",
        "- Check your **entitlements** (permissions) to see what actions you can perform, like creating clusters or managing users.\n",
        "- Import notebooks by right-clicking a folder > \"Import\" > upload a `.dbc`, `.py`, `.sql`, or other supported file.\n",
        "\n",
        "### Why It Matters\n",
        "Understanding the workspace layout helps you quickly find and manage resources, collaborate with your team, and stay organized during development and production.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Configure and Manage Databricks Clusters, Libraries, and Notebooks\n",
        "\n",
        "### Clusters\n",
        "- **What are they?** Clusters are groups of virtual machines (computers) that run your Databricks jobs. They consist of:\n",
        "  - **Driver Node:** The \"boss\" that coordinates tasks.\n",
        "  - **Worker Nodes:** The \"workers\" that process data in parallel.\n",
        "- **Types of Clusters:**\n",
        "  - **Single-node:** Only a driver node, ideal for small tasks like testing or lightweight development.\n",
        "  - **Multi-node:** Driver + multiple workers, used for big data processing or production jobs.\n",
        "- **Creating a Cluster:**\n",
        "  - Go to **Compute** > **Create Cluster** in the UI.\n",
        "  - Key settings:\n",
        "    - **Cluster Name:** Give it a meaningful name (e.g., `DataProcessing_Prod`).\n",
        "    - **Cluster Mode:**\n",
        "      - **Standard:** General-purpose, good for most tasks.\n",
        "      - **High Concurrency:** Optimized for multiple users sharing the cluster (e.g., SQL queries).\n",
        "    - **Databricks Runtime Version:** Choose a version (e.g., 14.3 LTS) for Spark and library compatibility.\n",
        "    - **Node Type:** Select VM size (e.g., Standard_DS3_v2) based on memory/CPU needs.\n",
        "    - **Autoscaling:** Enable to automatically add/remove workers based on workload (saves costs).\n",
        "    - **Spot Instances:** Use cheaper, interruptible VMs for non-critical jobs.\n",
        "    - **Auto-termination:** Set an idle time (e.g., 30 minutes) to shut down the cluster and save costs.\n",
        "- **Managing Clusters:**\n",
        "  - Start/stop clusters manually in the Compute tab.\n",
        "  - Use **cluster policies** (set by admins) to enforce rules, like limiting VM types or max workers.\n",
        "  - Monitor cluster metrics (CPU, memory) in the cluster UI to optimize performance.\n",
        "\n",
        "### Libraries\n",
        "- **What are they?** Libraries are external packages or dependencies (e.g., Python’s `pandas`, Java’s JAR files) that add functionality to your clusters.\n",
        "- **Installing Libraries:**\n",
        "  - In the cluster UI, go to **Libraries** tab > **Install New**.\n",
        "  - Types:\n",
        "    - **PyPI:** For Python packages (e.g., `requests==2.28.1`).\n",
        "    - **Maven:** For Java/Scala libraries (e.g., Spark connectors).\n",
        "    - **JAR/Wheel:** Upload custom files.\n",
        "  - **Workspace-level Libraries:** Install at the workspace level (via Libraries UI) to share across multiple clusters.\n",
        "- **Tip:** Restart the cluster after installing libraries to ensure they load properly.\n",
        "\n",
        "### Notebooks\n",
        "- **What are they?** Notebooks are interactive documents where you write code (Python, Scala, SQL, R), visualize results, and add comments using markdown.\n",
        "- **Key Actions:**\n",
        "  - **Create:** File > New Notebook, select language and cluster.\n",
        "  - **Attach to Cluster:** Use the dropdown at the top of the notebook to attach a running cluster.\n",
        "  - **Clone:** File > Clone to create a copy for testing without changing the original.\n",
        "  - **Permissions:** Right-click > Permissions to set who can view, edit, or run the notebook.\n",
        "  - **Run Cells:** Use `Shift+Enter` to execute code cells or `%run /path/to/notebook` to run another notebook.\n",
        "- **Tip:** Use markdown cells (`%md`) for documentation and `%sql` for SQL queries within notebooks.\n",
        "\n",
        "### Practical Tips\n",
        "- Use single-node clusters for development to save costs, multi-node for production.\n",
        "- Regularly check library versions for compatibility with your runtime.\n",
        "- Organize notebooks in folders and use clear names (e.g., `ETL_Pipeline_v1`).\n",
        "\n",
        "### Why It Matters\n",
        "Clusters, libraries, and notebooks are the backbone of running data processing tasks. Proper configuration ensures efficient, cost-effective, and reliable workflows.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Utilize Databricks CLI and REST API for Automation and Management\n",
        "\n",
        "### Databricks CLI\n",
        "- **What is it?** A command-line tool to manage Databricks resources (clusters, jobs, notebooks) without using the UI, perfect for automation scripts.\n",
        "- **Setup:**\n",
        "  - Install: Run `pip install databricks-cli` in your terminal.\n",
        "  - Configure: Run `databricks configure --token`, then provide:\n",
        "    - **Databricks Host:** Your workspace URL (e.g., `https://<region>.azuredatabricks.net`).\n",
        "    - **Token:** A personal access token from User Settings > Access Tokens.\n",
        "- **Common Commands:**\n",
        "  - List clusters: `databricks clusters list` (shows cluster IDs, names, statuses).\n",
        "  - Start cluster: `databricks clusters start --cluster-id <id>`.\n",
        "  - Upload notebook: `databricks workspace import --path /local/notebook.py --destination /Workspace/path`.\n",
        "  - Export folder: `databricks workspace export_dir /Workspace/folder /local/dir`.\n",
        "- **Tip:** Add `--output JSON` for structured output in scripts (e.g., for parsing in Python).\n",
        "\n",
        "### Databricks REST API\n",
        "- **What is it?** A programmatic way to interact with Databricks using HTTP requests, ideal for integrating with CI/CD pipelines or custom apps.\n",
        "- **Authentication:** Use a personal access token in the request header: `Authorization: Bearer <token>`.\n",
        "- **Key Endpoints:**\n",
        "  - **Clusters API:** `/api/2.0/clusters/list` to list clusters, `/api/2.0/clusters/start` to start one.\n",
        "  - **Jobs API:** `/api/2.1/jobs/create` to create a job, `/api/2.1/jobs/run-now` to trigger it.\n",
        "  - **Workspace API:** `/api/2.0/workspace/import` to upload notebooks, `/api/2.0/workspace/export` to download.\n",
        "  - **Secrets API:** `/api/2.0/secrets/put` to store sensitive data like API keys.\n",
        "- **Example (Python):**\n",
        "  ```python\n",
        "  import requests\n",
        "  url = \"https://<region>.azuredatabricks.net/api/2.0/clusters/list\"\n",
        "  headers = {\"Authorization\": \"Bearer <token>\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  print(response.json())\n",
        "  ```\n",
        "\n",
        "### Practical Tips\n",
        "- Use CLI for quick tasks like uploading notebooks in bulk.\n",
        "- Use REST API for complex automation, like triggering jobs from a CI/CD pipeline.\n",
        "- Store tokens securely (e.g., in environment variables) to avoid exposing them.\n",
        "\n",
        "### Why It Matters\n",
        "CLI and REST API enable automation, reducing manual work and ensuring consistency in managing large-scale Databricks environments.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Use dbutils for File and Dependency Management\n",
        "\n",
        "### What is dbutils?\n",
        "- `dbutils` is a built-in utility module in Databricks notebooks for tasks like file operations, secret management, and widget handling.\n",
        "- Available in Python, Scala, and R; access it with `dbutils.help()` to see available commands.\n",
        "\n",
        "### File Management with dbutils.fs\n",
        "- **Databricks File System (DBFS):** A distributed file system for storing data, notebooks, and libraries.\n",
        "- **Key Commands:**\n",
        "  - List files: `dbutils.fs.ls(\"/path/to/dir\")` – Returns file names, sizes, and paths.\n",
        "  - Copy files: `dbutils.fs.cp(\"/source\", \"/destination\", recurse=True)` – Copies files or folders.\n",
        "  - Move files: `dbutils.fs.mv(\"/source\", \"/destination\")` – Moves files or folders.\n",
        "  - Delete: `dbutils.fs.rm(\"/path\", recurse=True)` – Deletes files or directories.\n",
        "  - Mount storage: `dbutils.fs.mount(source=\"s3://bucket\", mountPoint=\"/mnt/dir\", extraConfigs={\"fs.s3a.access.key\": key})` – Connects cloud storage (e.g., S3, ADLS) to DBFS.\n",
        "- **Example:**\n",
        "  ```python\n",
        "  # List files in DBFS\n",
        "  display(dbutils.fs.ls(\"/mnt/data\"))\n",
        "  # Copy a file\n",
        "  dbutils.fs.cp(\"/mnt/data/input.csv\", \"/mnt/data/backup/input.csv\")\n",
        "  ```\n",
        "\n",
        "### Dependency Management with dbutils.widgets\n",
        "- **What are widgets?** Interactive inputs (e.g., text boxes, dropdowns) for passing parameters to notebooks.\n",
        "- **Key Commands:**\n",
        "  - Create: `dbutils.widgets.text(\"param_name\", \"default_value\", \"Label\")` – Creates a text input.\n",
        "  - Get value: `param = dbutils.widgets.get(\"param_name\")` – Retrieves the input value.\n",
        "  - Dropdown: `dbutils.widgets.dropdown(\"env\", \"prod\", [\"dev\", \"prod\"], \"Environment\")` – Creates a dropdown.\n",
        "- **Example:**\n",
        "  ```python\n",
        "  dbutils.widgets.text(\"table_name\", \"sales\", \"Table Name\")\n",
        "  table = dbutils.widgets.get(\"table_name\")\n",
        "  spark.sql(f\"SELECT * FROM {table}\").show()\n",
        "  ```\n",
        "\n",
        "### Secrets Management\n",
        "- **Purpose:** Securely store and access sensitive data (e.g., API keys, passwords).\n",
        "- **Commands:**\n",
        "  - Get secret: `dbutils.secrets.get(scope=\"my_scope\", key=\"my_key\")`.\n",
        "  - Scopes are created via CLI/API (e.g., `databricks secrets create-scope my_scope`).\n",
        "- **Example:**\n",
        "  ```python\n",
        "  api_key = dbutils.secrets.get(scope=\"api_scope\", key=\"api_key\")\n",
        "  ```\n",
        "\n",
        "### Practical Tips\n",
        "- Use `dbutils.fs.mount` to connect cloud storage once and access it like a local folder.\n",
        "- Use widgets for reusable notebooks that need dynamic inputs (e.g., table names).\n",
        "- Always use secrets for sensitive data instead of hardcoding in notebooks.\n",
        "\n",
        "### Why It Matters\n",
        "`dbutils` simplifies file operations and dependency management, making your notebooks more flexible and secure.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Implement Databricks Workflows for Job and Task Orchestration\n",
        "\n",
        "### What are Databricks Workflows?\n",
        "- Workflows are a way to automate and orchestrate multiple tasks (e.g., running notebooks, JARs, or SQL queries) in a specific order.\n",
        "- Unlike a single job (one task), workflows support complex pipelines with dependencies.\n",
        "\n",
        "### Creating a Workflow\n",
        "- **Steps:**\n",
        "  - Go to **Workflows** > **Create Job**.\n",
        "  - Add tasks:\n",
        "    - **Task Type:** Notebook, JAR, Python script, Delta Live Tables, etc.\n",
        "    - **Cluster:** Choose an existing cluster or create a new one.\n",
        "    - **Dependencies:** Set which tasks run after others (e.g., Task B depends on Task A).\n",
        "  - Name tasks clearly (e.g., `Extract_Data`, `Transform_Data`).\n",
        "- **Example Workflow:**\n",
        "  - Task 1: Extract data from a CSV (notebook).\n",
        "  - Task 2: Transform data with Spark (depends on Task 1).\n",
        "  - Task 3: Load to Delta table (depends on Task 2).\n",
        "\n",
        "### Scheduling Workflows\n",
        "- In the job UI, click **Schedule**:\n",
        "  - Use cron syntax (e.g., `0 0 * * *` for daily at midnight).\n",
        "  - Set time zone and pause/resume options.\n",
        "- **Tip:** Test the workflow manually before scheduling to ensure it runs correctly.\n",
        "\n",
        "### Advanced Features\n",
        "- **Task Values:** Pass data between tasks using `dbutils.jobs.taskValues.set(key, value)` and `{{tasks.task_name.values.key}}`.\n",
        "  ```python\n",
        "  # In Task 1\n",
        "  dbutils.jobs.taskValues.set(\"row_count\", 100)\n",
        "  # In Task 2\n",
        "  row_count = \"{{tasks.task1.values.row_count}}\"\n",
        "  ```\n",
        "- **Repair Runs:** Re-run failed tasks with modified parameters without restarting the entire workflow.\n",
        "- **Alerts:** Set email notifications for success, failure, or skipped tasks in the job settings.\n",
        "\n",
        "### Delta Live Tables (DLT) in Workflows\n",
        "- DLT is a special workflow type for building ETL pipelines using declarative SQL or Python.\n",
        "- Create via **Workflows** > **Delta Live Tables** > define datasets and transformations.\n",
        "- **Example:** Define a pipeline to clean and aggregate data into a Delta table.\n",
        "\n",
        "### Practical Tips\n",
        "- Break complex jobs into smaller tasks for easier debugging.\n",
        "- Use repair runs to fix failures without re-running successful tasks.\n",
        "- Monitor runs in the Workflows UI (logs, durations, and errors).\n",
        "\n",
        "### Why It Matters\n",
        "Workflows automate and orchestrate data pipelines, ensuring reliable execution and scalability for production jobs.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Manage Databricks Repos for Version Control and Collaboration\n",
        "\n",
        "### What are Databricks Repos?\n",
        "- Repos integrate Databricks with Git (e.g., GitHub, GitLab) for version control of notebooks, scripts, and other files.\n",
        "- They allow teams to collaborate, track changes, and manage code versions.\n",
        "\n",
        "### Setting Up Repos\n",
        "- **Steps:**\n",
        "  - Go to **Repos** in the sidebar > **Add Repo**.\n",
        "  - Provide:\n",
        "    - **Git URL:** Your repository URL (e.g., `https://github.com/user/repo`).\n",
        "    - **Provider:** GitHub, GitLab, Bitbucket, etc.\n",
        "    - **Branch:** The Git branch to sync (e.g., `main`).\n",
        "  - Authenticate with a Git token (generate in your Git provider’s settings).\n",
        "- **Result:** The repo appears as a folder in `/Repos/yourname/repo`.\n",
        "\n",
        "### Using Repos\n",
        "- **Clone and Sync:**\n",
        "  - Clone a Git repo into Databricks to work on notebooks and files.\n",
        "  - Pull updates: Right-click the repo > **Pull** to sync with the remote Git branch.\n",
        "  - Commit/push: Edit files, commit changes in the UI, and push to Git.\n",
        "- **Branching:**\n",
        "  - Create branches in the Repos UI to work on features or fixes.\n",
        "  - Switch branches via the repo’s branch dropdown.\n",
        "- **Collaboration:**\n",
        "  - Multiple users can work on the same repo, with Git managing conflicts.\n",
        "  - Use pull requests (in your Git provider) for code reviews before merging.\n",
        "\n",
        "### Practical Tips\n",
        "- Store notebooks as `.py` or `.sql` files in Repos for better Git compatibility (instead of `.dbc`).\n",
        "- Regularly pull updates to avoid conflicts with teammates’ changes.\n",
        "- Use Repos for CI/CD by integrating with GitHub Actions or similar tools.\n",
        "\n",
        "### Why It Matters\n",
        "Repos enable version control and collaboration, ensuring code quality and consistency in team projects.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "uSV_om7-PNTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dNSCozaJPtRw"
      }
    }
  ]
}